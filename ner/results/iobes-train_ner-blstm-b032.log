{
  "gpu": 0,
  "train": "datasets/train-iobes.txt",
  "valid": "datasets/test-iobes.txt",
  "test": "datasets/test-iobes.txt",
  "glove": "datasets/glove.6B.100d.txt",
  "w2v": "",
  "unit": 200,
  "batchsize": 32,
  "epoch": 30,
  "out": "results/iobes-result-blstm-b032"
}
2018-09-09 17:13:45,868 - main - INFO - vocabulary size: 19925
2018-09-09 17:13:45,868 - main - INFO - number of word embedding dims: 100
2018-09-09 17:13:45,868 - main - INFO - number of lstm units: 200
2018-09-09 17:13:45,868 - main - INFO - number of tags: 17
2018-09-09 17:13:45,868 - main - INFO - train data length: 14986
2018-09-09 17:13:45,869 - main - INFO - valid data length: 3683
2018-09-09 17:13:45,869 - main - INFO - test  data length: 3683
Initialize word embedding by pre-trained model: datasets/glove.6B.100d.txt
2018-09-09 17:15:28,501 - main - INFO - [  1] T/loss=8.200171 T/f1=0.263339 T/acc=0.840107 T/sec= 92.706947 V/loss=3.298893 V/f1=0.592474 V/acc=0.907080 V/sec= 9.648419 lr=0.015000
saving early stopped-model at epoch 1
2018-09-09 17:17:11,010 - main - INFO - [  2] T/loss=4.726076 T/f1=0.620277 T/acc=0.917841 T/sec= 92.837927 V/loss=2.325810 V/f1=0.692337 V/acc=0.925566 V/sec= 9.670679 lr=0.014925
saving early stopped-model at epoch 2
2018-09-09 17:18:53,452 - main - INFO - [  3] T/loss=4.006879 T/f1=0.745303 T/acc=0.942917 T/sec= 92.776686 V/loss=1.918898 V/f1=0.742633 V/acc=0.936984 V/sec= 9.665197 lr=0.014850
saving early stopped-model at epoch 3
2018-09-09 17:20:36,138 - main - INFO - [  4] T/loss=3.681901 T/f1=0.807396 T/acc=0.955399 T/sec= 93.020258 V/loss=1.715650 V/f1=0.757837 V/acc=0.941991 V/sec= 9.666283 lr=0.014776
saving early stopped-model at epoch 4
2018-09-09 17:22:19,055 - main - INFO - [  5] T/loss=3.376423 T/f1=0.847797 T/acc=0.963863 T/sec= 93.257329 V/loss=1.566311 V/f1=0.783894 V/acc=0.949164 V/sec= 9.658991 lr=0.014702
saving early stopped-model at epoch 5
2018-09-09 17:24:01,584 - main - INFO - [  6] T/loss=3.200469 T/f1=0.876447 T/acc=0.970330 T/sec= 92.863329 V/loss=1.543550 V/f1=0.782152 V/acc=0.948345 V/sec= 9.666484 lr=0.014629
2018-09-09 17:25:43,870 - main - INFO - [  7] T/loss=3.071036 T/f1=0.894715 T/acc=0.974016 T/sec= 92.529724 V/loss=1.552871 V/f1=0.785727 V/acc=0.949409 V/sec= 9.756253 lr=0.014556
saving early stopped-model at epoch 7
2018-09-09 17:27:26,683 - main - INFO - [  8] T/loss=2.939665 T/f1=0.907842 T/acc=0.977240 T/sec= 93.124325 V/loss=1.473039 V/f1=0.797434 V/acc=0.952343 V/sec= 9.688790 lr=0.014483
saving early stopped-model at epoch 8
2018-09-09 17:29:09,564 - main - INFO - [  9] T/loss=2.926740 T/f1=0.919016 T/acc=0.980119 T/sec= 93.196610 V/loss=1.453820 V/f1=0.804718 V/acc=0.954421 V/sec= 9.684250 lr=0.014410
saving early stopped-model at epoch 9
2018-09-09 17:30:52,471 - main - INFO - [ 10] T/loss=2.807651 T/f1=0.925927 T/acc=0.981967 T/sec= 93.226644 V/loss=1.440132 V/f1=0.810131 V/acc=0.955319 V/sec= 9.680561 lr=0.014338
saving early stopped-model at epoch 10
2018-09-09 17:32:35,155 - main - INFO - [ 11] T/loss=2.731512 T/f1=0.932373 T/acc=0.983703 T/sec= 93.006259 V/loss=1.531368 V/f1=0.788691 V/acc=0.950027 V/sec= 9.677577 lr=0.014267
2018-09-09 17:34:17,414 - main - INFO - [ 12] T/loss=2.722507 T/f1=0.938587 T/acc=0.985144 T/sec= 92.537775 V/loss=1.426504 V/f1=0.808637 V/acc=0.955355 V/sec= 9.721212 lr=0.014195
2018-09-09 17:35:59,792 - main - INFO - [ 13] T/loss=2.650924 T/f1=0.942779 T/acc=0.986147 T/sec= 92.671869 V/loss=1.421367 V/f1=0.812031 V/acc=0.956306 V/sec= 9.705769 lr=0.014124
saving early stopped-model at epoch 13
2018-09-09 17:37:42,974 - main - INFO - [ 14] T/loss=2.608010 T/f1=0.947144 T/acc=0.987250 T/sec= 93.449285 V/loss=1.578566 V/f1=0.806064 V/acc=0.954812 V/sec= 9.732916 lr=0.014054
2018-09-09 17:39:24,858 - main - INFO - [ 15] T/loss=2.575253 T/f1=0.950646 T/acc=0.988124 T/sec= 92.188419 V/loss=1.466322 V/f1=0.815633 V/acc=0.956677 V/sec= 9.695654 lr=0.013983
saving early stopped-model at epoch 15
2018-09-09 17:41:08,279 - main - INFO - [ 16] T/loss=2.565080 T/f1=0.954226 T/acc=0.989067 T/sec= 93.632048 V/loss=1.526326 V/f1=0.810620 V/acc=0.955216 V/sec= 9.789171 lr=0.013914
2018-09-09 17:42:50,704 - main - INFO - [ 17] T/loss=2.511078 T/f1=0.956582 T/acc=0.989795 T/sec= 92.743260 V/loss=1.750902 V/f1=0.802285 V/acc=0.952509 V/sec= 9.681609 lr=0.013844
2018-09-09 17:44:32,936 - main - INFO - [ 18] T/loss=2.531468 T/f1=0.956353 T/acc=0.989580 T/sec= 92.457155 V/loss=1.605882 V/f1=0.806656 V/acc=0.954900 V/sec= 9.774621 lr=0.013775
2018-09-09 17:46:15,434 - main - INFO - [ 19] T/loss=2.482712 T/f1=0.958136 T/acc=0.990286 T/sec= 92.735016 V/loss=1.631697 V/f1=0.811746 V/acc=0.956542 V/sec= 9.763131 lr=0.013706
2018-09-09 17:47:57,863 - main - INFO - [ 20] T/loss=2.441606 T/f1=0.960633 T/acc=0.990850 T/sec= 92.464950 V/loss=1.710187 V/f1=0.805327 V/acc=0.954330 V/sec= 9.963788 lr=0.013637
2018-09-09 17:49:39,869 - main - INFO - [ 21] T/loss=2.444900 T/f1=0.963762 T/acc=0.991545 T/sec= 92.324884 V/loss=1.694725 V/f1=0.808323 V/acc=0.954412 V/sec= 9.681268 lr=0.013569
/Users/haradatm/.pyenv/versions/miniconda3-latest/envs/py3/lib/python3.6/site-packages/matplotlib/pyplot.py:537: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  max_open_warning, RuntimeWarning)
2018-09-09 17:51:22,255 - main - INFO - [ 22] T/loss=2.446215 T/f1=0.964559 T/acc=0.991642 T/sec= 92.692884 V/loss=1.834814 V/f1=0.804627 V/acc=0.953509 V/sec= 9.693336 lr=0.013501
2018-09-09 17:53:05,245 - main - INFO - [ 23] T/loss=2.439656 T/f1=0.966589 T/acc=0.992327 T/sec= 93.245147 V/loss=1.671011 V/f1=0.814525 V/acc=0.956400 V/sec= 9.744333 lr=0.013434
2018-09-09 17:54:47,918 - main - INFO - [ 24] T/loss=2.391636 T/f1=0.965036 T/acc=0.991998 T/sec= 92.961749 V/loss=1.721117 V/f1=0.809645 V/acc=0.954995 V/sec= 9.711555 lr=0.013367
2018-09-09 17:56:30,492 - main - INFO - [ 25] T/loss=2.363294 T/f1=0.967030 T/acc=0.992491 T/sec= 92.841779 V/loss=1.816386 V/f1=0.803581 V/acc=0.953246 V/sec= 9.731640 lr=0.013300
2018-09-09 17:58:13,329 - main - INFO - [ 26] T/loss=2.361703 T/f1=0.967536 T/acc=0.992726 T/sec= 93.080976 V/loss=1.861448 V/f1=0.802980 V/acc=0.951780 V/sec= 9.756447 lr=0.013233
2018-09-09 17:59:55,963 - main - INFO - [ 27] T/loss=2.386791 T/f1=0.969721 T/acc=0.993322 T/sec= 92.863317 V/loss=1.942066 V/f1=0.805894 V/acc=0.953394 V/sec= 9.771106 lr=0.013167
2018-09-09 18:01:38,669 - main - INFO - [ 28] T/loss=2.399126 T/f1=0.968955 T/acc=0.993022 T/sec= 92.969143 V/loss=1.718852 V/f1=0.807536 V/acc=0.955501 V/sec= 9.735981 lr=0.013101
2018-09-09 18:03:21,253 - main - INFO - [ 29] T/loss=2.349786 T/f1=0.970500 T/acc=0.993493 T/sec= 92.859970 V/loss=2.099698 V/f1=0.783744 V/acc=0.947582 V/sec= 9.724969 lr=0.013036
2018-09-09 18:05:04,141 - main - INFO - [ 30] T/loss=2.394020 T/f1=0.972153 T/acc=0.993863 T/sec= 93.162972 V/loss=1.664761 V/f1=0.809514 V/acc=0.954950 V/sec= 9.724496 lr=0.012971
loading early stopped-model at epoch 15
             precision    recall  f1-score   support

       MISC       0.83      0.67      0.74       702
        ORG       0.85      0.69      0.76      1661
        LOC       0.87      0.87      0.87      1668
        PER       0.95      0.83      0.89      1617

avg / total       0.88      0.78      0.83      5648

2018-09-09 18:05:07,214 - <module> - INFO - time spent: 3143.983434 sec

