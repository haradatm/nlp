{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_rnnlm-1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"V7RcFkFmHJqx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"e6c336d7-7941-48b3-f36c-22fa92b4b1b1","executionInfo":{"status":"ok","timestamp":1533954983966,"user_tz":-540,"elapsed":1939,"user":{"displayName":"HARADA Tomohiko","photoUrl":"//lh3.googleusercontent.com/-NhVNZogeGbU/AAAAAAAAAAI/AAAAAAAAAB4/SUNHPkvUldw/s50-c-k-no/photo.jpg","userId":"102955192351597992804"}}},"cell_type":"code","source":["!df -h"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Filesystem      Size  Used Avail Use% Mounted on\r\n","overlay         359G  6.3G  334G   2% /\r\n","tmpfs           6.4G     0  6.4G   0% /dev\r\n","tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\r\n","tmpfs           6.4G  249M  6.2G   4% /opt/bin\r\n","/dev/sda1       365G  8.1G  357G   3% /etc/hosts\r\n","shm              64M     0   64M   0% /dev/shm\r\n","tmpfs           6.4G     0  6.4G   0% /sys/firmware\r\n"],"name":"stdout"}]},{"metadata":{"id":"wBcsYE8hau2J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e310c468-9f2e-49a0-8e44-4129b5a4d99d","executionInfo":{"status":"ok","timestamp":1534480021437,"user_tz":-540,"elapsed":1775,"user":{"displayName":"HARADA Tomohiko","photoUrl":"//lh3.googleusercontent.com/-NhVNZogeGbU/AAAAAAAAAAI/AAAAAAAAAB4/SUNHPkvUldw/s50-c-k-no/photo.jpg","userId":"102955192351597992804"}}},"cell_type":"code","source":["# 起動時間\n","!cat /proc/uptime | awk '{print $1 /60 /60 /24 \"days (\" $1 \"sec)\"}'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.0178927days (1545.93sec)\r\n"],"name":"stdout"}]},{"metadata":{"id":"pcsST61uGqpt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"outputId":"62343241-e2db-4b1a-9de3-e69941138db3","executionInfo":{"status":"ok","timestamp":1534595671140,"user_tz":-540,"elapsed":11285,"user":{"displayName":"HARADA Tomohiko","photoUrl":"//lh3.googleusercontent.com/-NhVNZogeGbU/AAAAAAAAAAI/AAAAAAAAAB4/SUNHPkvUldw/s50-c-k-no/photo.jpg","userId":"102955192351597992804"}}},"cell_type":"code","source":["!apt -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n","!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n","!pip install -U cupy_cuda80\n","!pip install -U chainer"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libcusparse8.0 is already the newest version (8.0.61-1).\n","libnvrtc8.0 is already the newest version (8.0.61-1).\n","libnvtoolsext1 is already the newest version (8.0.61-1).\n","0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n","Requirement already up-to-date: cupy_cuda80 in /usr/local/lib/python3.6/dist-packages (4.3.0)\n","Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy_cuda80) (1.11.0)\n","Requirement already satisfied, skipping upgrade: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy_cuda80) (0.3)\n","Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy_cuda80) (1.14.5)\n","Requirement already up-to-date: chainer in /usr/local/lib/python3.6/dist-packages (4.3.1)\n","Requirement already satisfied, skipping upgrade: cupy-cuda80<5.0.0,>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from chainer) (4.3.0)\n","Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from chainer) (3.0.4)\n","Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer) (1.11.0)\n","Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer) (1.14.5)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from chainer) (3.6.1)\n","Requirement already satisfied, skipping upgrade: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda80<5.0.0,>=4.3.0->chainer) (0.3)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.0.0->chainer) (39.1.0)\n"],"name":"stdout"}]},{"metadata":{"id":"RsZae1iCGTZu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"0b010d92-7efd-462a-cf05-74728c6fe4f8","executionInfo":{"status":"ok","timestamp":1534595675540,"user_tz":-540,"elapsed":710,"user":{"displayName":"HARADA Tomohiko","photoUrl":"//lh3.googleusercontent.com/-NhVNZogeGbU/AAAAAAAAAAI/AAAAAAAAAB4/SUNHPkvUldw/s50-c-k-no/photo.jpg","userId":"102955192351597992804"}}},"cell_type":"code","source":["import chainer\n","print('GPU availability:', chainer.cuda.available)\n","print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["GPU availability: True\n","cuDNN availablility: True\n"],"name":"stdout"}]},{"metadata":{"id":"HjTK9-YEH3Lr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"099f3ff3-6cf1-4923-9356-32c5f49c8340","executionInfo":{"status":"ok","timestamp":1534595692262,"user_tz":-540,"elapsed":13314,"user":{"displayName":"HARADA Tomohiko","photoUrl":"//lh3.googleusercontent.com/-NhVNZogeGbU/AAAAAAAAAAI/AAAAAAAAAB4/SUNHPkvUldw/s50-c-k-no/photo.jpg","userId":"102955192351597992804"}}},"cell_type":"code","source":["# google-drive-ocamlfuse のインストール\n","!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse"],"execution_count":0,"outputs":[{"output_type":"stream","text":["gpg: keybox '/tmp/tmp3dduj01p/pubring.gpg' created\n","gpg: /tmp/tmp3dduj01p/trustdb.gpg: trustdb created\n","gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n"],"name":"stdout"}]},{"metadata":{"id":"r-rQdmjRIArz","colab_type":"code","colab":{}},"cell_type":"code","source":["# Colab 用の Auth token 成\n","from google.colab import auth\n","auth.authenticate_user()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oVhqR8gaIYYf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":933},"outputId":"29a4cd33-ca45-4a42-8d77-b529dd6b708d","executionInfo":{"status":"error","timestamp":1534595737443,"user_tz":-540,"elapsed":8067,"user":{"displayName":"HARADA Tomohiko","photoUrl":"//lh3.googleusercontent.com/-NhVNZogeGbU/AAAAAAAAAAI/AAAAAAAAAB4/SUNHPkvUldw/s50-c-k-no/photo.jpg","userId":"102955192351597992804"}}},"cell_type":"code","source":["# Drive FUSE library 用の credential 生成\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-b9a81c740bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"gYBRyqeiHWuB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2041fba8-2f53-4e80-a863-4351c585be54","executionInfo":{"status":"ok","timestamp":1534595745438,"user_tz":-540,"elapsed":5017,"user":{"displayName":"HARADA Tomohiko","photoUrl":"//lh3.googleusercontent.com/-NhVNZogeGbU/AAAAAAAAAAI/AAAAAAAAAB4/SUNHPkvUldw/s50-c-k-no/photo.jpg","userId":"102955192351597992804"}}},"cell_type":"code","source":["# drive/ を作り, そこに Google Drive をマウントする\n","!mkdir -p drive\n","!google-drive-ocamlfuse drive\n","!ls \"drive/Colab Notebooks/kenkyu\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["180715\t180818\r\n"],"name":"stdout"}]},{"metadata":{"id":"PywN5Q0fTxLJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"eb2cfd96-5d22-48c1-9f1e-d59a5050c09a","executionInfo":{"status":"ok","timestamp":1533955114044,"user_tz":-540,"elapsed":2186,"user":{"displayName":"HARADA Tomohiko","photoUrl":"//lh3.googleusercontent.com/-NhVNZogeGbU/AAAAAAAAAAI/AAAAAAAAAB4/SUNHPkvUldw/s50-c-k-no/photo.jpg","userId":"102955192351597992804"}}},"cell_type":"code","source":["!pip freeze | grep nltk"],"execution_count":0,"outputs":[{"output_type":"stream","text":["nltk==3.2.5\r\n"],"name":"stdout"}]},{"metadata":{"id":"DJpcv2-oT2Yy","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install -U nltk==3.2.5"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YRzrn2xJdvl9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"58a0f1fe-6a98-4683-8745-7ba7041b7543","executionInfo":{"status":"ok","timestamp":1534595750571,"user_tz":-540,"elapsed":682,"user":{"displayName":"HARADA Tomohiko","photoUrl":"//lh3.googleusercontent.com/-NhVNZogeGbU/AAAAAAAAAAI/AAAAAAAAAB4/SUNHPkvUldw/s50-c-k-no/photo.jpg","userId":"102955192351597992804"}}},"cell_type":"code","source":["cd \"~/drive/Colab Notebooks/kenkyu/180818\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/Colab Notebooks/kenkyu/180818\n"],"name":"stdout"}]},{"metadata":{"id":"4hBipaWUd00w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"23369055-625f-4ddc-a6c2-99dd8b9a7a6c","executionInfo":{"status":"ok","timestamp":1534595757979,"user_tz":-540,"elapsed":1653,"user":{"displayName":"HARADA Tomohiko","photoUrl":"//lh3.googleusercontent.com/-NhVNZogeGbU/AAAAAAAAAAI/AAAAAAAAAB4/SUNHPkvUldw/s50-c-k-no/photo.jpg","userId":"102955192351597992804"}}},"cell_type":"code","source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["datasets  drive  train_rnnlm-1.ipynb  train_rnnlm-2.ipynb\r\n"],"name":"stdout"}]},{"metadata":{"id":"cU8n_IaKE__X","colab_type":"code","colab":{}},"cell_type":"code","source":["#!/usr/bin/env python\n","# -*- coding: utf-8 -*-\n","\n","\"\"\" Sample script of recurrent neural network language model. (using NStep-LSTM)\n","\n","    usage: python3.6 train_rnnlm.py --gpu -1 --epoch 200 --batchsize 100 --unit 300 --train datasets/soseki/neko-word-train.txt --test datasets/soseki/neko-word-test.txt --w2v datasets/soseki/neko_w2v.bin --out model-neko\n","    usage: python3.6  test_rnnlm.py --gpu -1 --model \"model-neko/final.model\" --text \"吾輩 は 猫 で ある 。\"\n","\"\"\"\n","\n","__version__ = '0.0.1'\n","\n","import sys, os, time, logging, json\n","import numpy as np\n","np.set_printoptions(precision=20)\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.DEBUG)\n","handler = logging.StreamHandler()\n","# handler = logging.FileHandler(filename=\"log.txt\")\n","handler.setFormatter(logging.Formatter('%(asctime)s - %(funcName)s - %(levelname)s - %(message)s'))\n","handler.setLevel(logging.DEBUG)\n","logger.addHandler(handler)\n","\n","def pp(obj):\n","    import pprint\n","    pp = pprint.PrettyPrinter(indent=1, width=160)\n","    logger.info(pp.pformat(obj))\n","\n","\n","start_time = time.time()\n","\n","\n","import chainer\n","from chainer import cuda\n","import chainer.functions as F\n","import chainer.links as L\n","import matplotlib.pyplot as plt\n","import pickle\n","from struct import unpack, calcsize\n","from sklearn.utils import shuffle as skshuffle\n","\n","# UNK_ID = 0\n","# EOS_ID = 1\n","# UNK_TOKEN = '<unk>'\n","EOS_TOKEN = '</s>'\n","\n","prime_text = \"\"\n","\n","\n","def load_w2v_model(path, vocab=[]):\n","\n","    with open(path, 'rb') as f:\n","\n","        n_vocab, n_units = map(int, f.readline().split())\n","        M = np.empty((n_vocab, n_units), dtype=np.float32)\n","\n","        for i in range(n_vocab):\n","            b_str = b''\n","\n","            while True:\n","                b_ch = f.read(1)\n","                if b_ch == b' ':\n","                    break\n","                b_str += b_ch\n","\n","            token = b_str.decode(encoding='utf-8')\n","\n","            if token not in vocab:\n","                vocab += [token]\n","            else:\n","                logging.error(\"Duplicate token: {}\", token)\n","\n","            M[i] = np.zeros(n_units)\n","            for j in range(n_units):\n","                M[i][j] = unpack('f', f.read(calcsize('f')))[0]\n","\n","            # ベクトルを正規化する\n","            vlen = np.linalg.norm(M[i], 2)\n","            M[i] /= vlen\n","\n","            # 改行を strip する\n","            assert f.read(1) != '\\n'\n","\n","    return M, vocab\n","\n","\n","def load_data(filename, w2v, vocab, train=True):\n","    global prime_text\n","\n","    dataset = []\n","\n","    for i, line in enumerate(open(filename, 'r')):\n","        # if i > 100:\n","        #     break\n","\n","        line = line.strip()\n","        tokens = line.split(' ') + [EOS_TOKEN]\n","\n","        if i == 0 and train:\n","            prime_text = line.split(' ')\n","\n","        array = []\n","        for token in tokens:\n","            if token == '':\n","                continue\n","\n","            if train:\n","                if token not in vocab:\n","                    vocab += [token]\n","                    if w2v is not None:\n","                        v = np.random.uniform(-0.1, 0.1, (1, w2v.shape[1])).astype(np.float32)\n","                        v /= np.linalg.norm(v, 2)\n","                        w2v = np.vstack((w2v, v))\n","                array.append(vocab.index(token))\n","            else:\n","                if token in vocab:\n","                    array.append(vocab.index(token))\n","\n","        dataset.append(xp.array(array, dtype=np.int32))\n","\n","    return dataset, w2v, vocab\n","\n","\n","def sequence_embed(embed, xs):\n","    x_len = [len(x) for x in xs]\n","    x_section = np.cumsum(x_len[:-1])\n","    ex = embed(F.concat(xs, axis=0))\n","    exs = F.split_axis(ex, x_section, 0)\n","    return exs\n","\n","\n","class RNNLM(chainer.Chain):\n","\n","    def __init__(self, n_layers, n_vocab, n_units):\n","        super(RNNLM, self).__init__()\n","        with self.init_scope():\n","            self.embed = L.EmbedID(n_vocab, n_units)\n","            self.l1 = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n","            self.l2 = L.Linear(n_units, n_vocab)\n","\n","        for param in self.params():\n","            param.data[...] = np.random.uniform(-0.1, 0.1, param.data.shape)\n","\n","        self.n_layers = n_layers\n","        self.n_units = n_units\n","\n","    def __call__(self, xs, ys):\n","        hx, cx, os = self.forward(xs)\n","        concat_os = self.l2(F.concat(os, axis=0))\n","        concat_ys_out = F.concat(ys, axis=0)\n","\n","        batch = len(xs)\n","        n_words = concat_ys_out.shape[0]\n","\n","        loss = F.sum(F.softmax_cross_entropy(concat_os, concat_ys_out, reduce='no')) / batch\n","        accuracy = F.accuracy(concat_os, concat_ys_out)\n","        perplexity = xp.exp(loss.data * batch / n_words)\n","\n","        return loss, accuracy, perplexity\n","\n","    def forward(self, xs, hx=None, cx=None):\n","        exs = sequence_embed(self.embed, xs)\n","        hx, cx, ys = self.l1(hx=hx, cx=cx, xs=exs)\n","        return hx, cx, ys\n","\n","    def predict(self, xs, hx=None, cx=None):\n","        hx, cx, os = self.forward(xs, hx=hx, cx=cx)\n","        y = self.l2(F.concat(os, axis=0))\n","        return hx, cx, F.softmax(y)\n","\n","    def set_word_embedding(self, data):\n","        self.embed.W.data = data\n","\n","\n","def batch_iter(data, batch_size, shuffle=True):\n","    batch = []\n","    shuffled_data = np.copy(data)\n","    if shuffle:\n","        shuffled_data = skshuffle(shuffled_data)\n","    for line in shuffled_data:\n","        batch.append(line)\n","        if len(batch) == batch_size:\n","            yield batch\n","            batch = []\n","    if batch:\n","        yield batch\n","\n","\n","def show_sample(model, vocab, token2id, length=20, eos=EOS_TOKEN):\n","    for token in prime_text:\n","        sys.stdout.write(token)\n","\n","    hx, cx, prev_word = model.predict([xp.array([token2id[x] for x in prime_text], dtype=np.int32)])\n","\n","    for i in range(length):\n","        next_prob = cuda.to_cpu(prev_word.data)[-1].astype(np.float64)\n","        next_prob /= np.sum(next_prob)\n","        idx = np.random.choice(range(len(next_prob)), p=next_prob)\n","\n","        if vocab[idx] == EOS_TOKEN:\n","            sys.stdout.write(eos)\n","        else:\n","            sys.stdout.write(vocab[idx])\n","        hx, cx, prev_word = model.predict([xp.array([idx], dtype=np.int32)], hx=hx, cx=cx)\n","\n","    sys.stdout.write('\\n')\n","    sys.stdout.flush()\n","\n","\n","def main():\n","    global xp\n","\n","    import argparse\n","    parser = argparse.ArgumentParser(description='Chainer example: NStep RNNLM')\n","    parser.add_argument('--train', default='datasets/soseki/neko-word-train.txt', type=str, help='dataset to train (.txt)')\n","    parser.add_argument('--test', default='datasets/soseki/neko-word-test.txt', type=str, help='use tiny datasets to evaluate (.txt)')\n","    parser.add_argument('--w2v', '-w', default='datasets/soseki/neko_w2v.bin', type=str, help='initialize word embedding layer with word2vec (.bin)')\n","    parser.add_argument('--batchsize', '-b', type=int, default=100, help='number of sentence pairs in each mini-batch')\n","    parser.add_argument('--epoch', '-e', type=int, default=300, help='number of sweeps over the dataset to train')\n","    parser.add_argument('--unit', '-u', type=int, default=200, help='number of LSTM units in each layer')\n","    parser.add_argument('--layer', '-l', type=int, default=3, help='number of layers')\n","    parser.add_argument('--gpu', '-g', type=int, default=0, help='GPU ID (negative value indicates CPU)')\n","    parser.add_argument('--gradclip', '-c', type=float, default=5, help='gradient norm threshold to clip')\n","    parser.add_argument('--out', '-o', default='results_rnnlm-1', help='directory to output the result')\n","    parser.add_argument('--resume', '-r', default='', help='resume the training from snapshot')\n","    args = parser.parse_args(args=[])\n","#     args = parser.parse_args()\n","    print(json.dumps(args.__dict__, indent=2))\n","    sys.stdout.flush()\n","\n","    if args.gpu >= 0:\n","        cuda.get_device_from_id(args.gpu).use()\n","\n","    xp = cuda.cupy if args.gpu >= 0 else np\n","    xp.random.seed(123)\n","\n","    w2v, vocab, n_dims = None, [], args.unit\n","\n","    if args.w2v:\n","        w2v, vocab = load_w2v_model(args.w2v)\n","        n_dims = w2v.shape[1]\n","\n","    if args.test:\n","        train_data, w2v, vocab = load_data(args.train, w2v, vocab, train=True)\n","        test_data,  w2v, vocab = load_data(args.test,  w2v, vocab, train=False)\n","    else:\n","        dataset, w2v, vocab = load_data(args.train, w2v, vocab, train=True)\n","        train_data = dataset[:-100]\n","        test_data  = dataset[-100:]\n","\n","    token2id = {w: i for i, w in enumerate(vocab)}\n","\n","    logger.info('vocabulary size: %d' % len(vocab))\n","    logger.info('train data size: %d' % len(train_data))\n","    logger.info('train data starts with: {} ...'.format(' '.join(prime_text)))\n","    logger.info('test  data size: %d' % len(test_data))\n","    sys.stdout.flush()\n","\n","    if not os.path.exists(args.out):\n","        os.mkdir(args.out)\n","\n","    with open(os.path.join(args.out, 'vocab.bin'), 'wb') as f:\n","        pickle.dump(vocab, f)\n","\n","    model = RNNLM(args.layer, len(vocab), n_dims)\n","\n","    # 学習率\n","    lr = 0.0007\n","\n","    # 重み減衰\n","    # decay = 0.0001\n","    decay = 0.0005\n","\n","    # 学習率の減衰\n","    lr_decay = 0.995\n","\n","    # Setup optimizer (Optimizer の設定)\n","    optimizer = chainer.optimizers.Adam(alpha=lr)\n","    # optimizer = optimizers.AdaDelta()\n","    optimizer.setup(model)\n","    optimizer.add_hook(chainer.optimizer.GradientClipping(args.gradclip))\n","    optimizer.add_hook(chainer.optimizer.WeightDecay(decay))\n","\n","    # Resume the training from snapshot\n","    if args.resume:\n","        print('Resume the training from snapshot: {0}.{{model,state}}'.format(args.resume))\n","        chainer.serializers.load_npz('{}.model'.format(args.resume), model)\n","        chainer.serializers.load_npz('{}.state'.format(args.resume), optimizer, strict=False)\n","        sys.stdout.flush()\n","\n","    # Initialize word embedding layer with word2vec\n","    if not args.resume and args.w2v:\n","        print('Initialize the embedding from word2vec model: {}'.format(args.w2v))\n","        model.set_word_embedding(w2v)\n","        sys.stdout.flush()\n","\n","    if args.gpu >= 0:\n","        model.to_gpu(args.gpu)\n","\n","    # プロット用に実行結果を保存する\n","    train_loss = []\n","    train_accuracy1 = []\n","    train_accuracy2 = []\n","    test_loss = []\n","    test_accuracy1 = []\n","    test_accuracy2 = []\n","    min_loss = float('inf')\n","    min_epoch = 0\n","\n","    # 最初の時間情報を取得する\n","    start_at = time.time()\n","    cur_at = start_at\n","\n","    # Learning loop\n","    for epoch in range(1, args.epoch + 1):\n","\n","        # logger.info('epoch {:} / {:}'.format(epoch, n_epoch))\n","        # handler1.flush()\n","\n","        # training\n","        train_iter = batch_iter(train_data, args.batchsize)\n","        sum_train_loss = 0.\n","        sum_train_accuracy1 = 0.\n","        sum_train_accuracy2 = 0.\n","        K = 0\n","\n","        for xs in train_iter:\n","            x_batch = [(x[:-1]) for x in xs]\n","            y_batch = [(x[1:])  for x in xs]\n","\n","            # 順伝播させて誤差と精度を算出\n","            loss, accuracy, perp = model(x_batch, y_batch)\n","            sum_train_loss += float(loss.data) * len(y_batch)\n","            sum_train_accuracy1 += float(accuracy.data) * len(y_batch)\n","            sum_train_accuracy2 += float(perp) * len(y_batch)\n","            K += len(y_batch)\n","\n","            # 誤差逆伝播で勾配を計算 (minibatch ごと)\n","            model.cleargrads()\n","            loss.backward()\n","            optimizer.update()\n","\n","        # 訓練データの誤差と,正解精度を表示\n","        mean_train_loss = sum_train_loss / K\n","        mean_train_accuracy1 = sum_train_accuracy1 / K\n","        mean_train_accuracy2 = sum_train_accuracy2 / K\n","        train_loss.append(mean_train_loss)\n","        train_accuracy1.append(mean_train_accuracy1)\n","        train_accuracy2.append(mean_train_accuracy2)\n","        now = time.time()\n","        train_throughput = now - cur_at\n","        cur_at = now\n","\n","        # evaluation\n","        test_iter = batch_iter(test_data, args.batchsize)\n","        sum_test_loss = 0.\n","        sum_test_accuracy1 = 0.\n","        sum_test_accuracy2 = 0.\n","        K = 0\n","\n","        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n","            for xs in test_iter:\n","                x_batch = [(x[:-1]) for x in xs]\n","                y_batch = [(x[1:]) for x in xs]\n","\n","                # 順伝播させて誤差と精度を算出\n","                loss, accuracy, perp = model(x_batch, y_batch)\n","                sum_test_loss += float(loss.data) * len(y_batch)\n","                sum_test_accuracy1 += float(accuracy.data) * len(y_batch)\n","                sum_test_accuracy2 += float(perp) * len(y_batch)\n","                K += len(y_batch)\n","\n","        # テストデータでの誤差と正解精度を表示\n","        mean_test_loss = sum_test_loss / K\n","        mean_test_accuracy1 = sum_test_accuracy1 / K\n","        mean_test_accuracy2 = sum_test_accuracy2 / K\n","        test_loss.append(mean_test_loss)\n","        test_accuracy1.append(mean_test_accuracy1)\n","        test_accuracy2.append(mean_test_accuracy2)\n","        now = time.time()\n","        test_throughput = now - cur_at\n","\n","        logger.info(''\n","                    '[{:>3d}] '\n","                    'T/loss={:.6f} '\n","                    'T/acc={:.6f} '\n","                    'T/perp={:.6f} '\n","                    'T/sec= {:.6f} '\n","                    'D/loss={:.6f} '\n","                    'D/acc={:.6f} '\n","                    'D/perp={:.6f} '\n","                    'D/sec= {:.6f} '\n","                    'lr={:.6f}'\n","                    ''.format(\n","            epoch,\n","            mean_train_loss,\n","            mean_train_accuracy1,\n","            mean_train_accuracy2,\n","            train_throughput,\n","            mean_test_loss,\n","            mean_test_accuracy1,\n","            mean_test_accuracy2,\n","            test_throughput,\n","            optimizer.alpha)\n","        )\n","        sys.stdout.flush()\n","\n","        # model と optimizer を保存する\n","        if mean_train_loss < min_loss:\n","            min_loss = mean_train_loss\n","            min_epoch = epoch\n","            if args.gpu >= 0: model.to_cpu()\n","            chainer.serializers.save_npz(os.path.join(args.out, 'early_stopped.model'), model)\n","            chainer.serializers.save_npz(os.path.join(args.out, 'early_stopped.state'), optimizer)\n","            if args.gpu >= 0: model.to_gpu()\n","\n","        print(\"SAMPLE #=> \", end='')\n","        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n","            show_sample(model.copy(), vocab, token2id)\n","        sys.stdout.flush()\n","\n","        # 精度と誤差をグラフ描画\n","        if True:\n","            ylim1 = [min(train_loss + train_accuracy2 + test_loss + test_accuracy2), max(train_loss + train_accuracy2 + test_loss + test_accuracy2)]\n","            ylim2 = [min(train_accuracy1 + test_accuracy1), max(train_accuracy1 + test_accuracy1)]\n","\n","            # グラフ左\n","            plt.figure(figsize=(10, 10))\n","            plt.subplot(1, 2, 1)\n","            plt.ylim(ylim1)\n","            plt.plot(range(1, len(train_loss) + 1), train_loss, 'b')\n","            plt.plot(range(1, len(train_accuracy2) + 1), train_accuracy2, 'm')\n","            plt.grid(False)\n","            plt.ylabel('loss and perplexity')\n","            plt.legend(['train loss', 'train perplexity'], loc=\"lower left\")\n","            plt.twinx()\n","            plt.ylim(ylim2)\n","            plt.plot(range(1, len(train_accuracy1) + 1), train_accuracy1, 'r')\n","            plt.grid(False)\n","            # plt.ylabel('accuracy')\n","            plt.legend(['train accuracy'], loc=\"upper right\")\n","            plt.title('Loss and accuracy of train.')\n","\n","            # グラフ右\n","            plt.subplot(1, 2, 2)\n","            plt.ylim(ylim1)\n","            plt.plot(range(1, len(test_loss) + 1), test_loss, 'b')\n","            plt.plot(range(1, len(test_accuracy2) + 1), test_accuracy2, 'm')\n","            plt.grid(False)\n","            # plt.ylabel('loss and perplexity')\n","            plt.legend(['valid loss', 'valid perplexity'], loc=\"lower left\")\n","            plt.twinx()\n","            plt.ylim(ylim2)\n","            plt.plot(range(1, len(test_accuracy1) + 1), test_accuracy1, 'r')\n","            plt.grid(False)\n","            plt.ylabel('accuracy')\n","            plt.legend(['valid accuracy'], loc=\"upper right\")\n","            plt.title('Loss and accuracy of valid.')\n","\n","            plt.savefig('{}.png'.format(args.out))\n","#             plt.savefig('{}.png'.format(os.path.splitext(os.path.basename(__file__))[0]))\n","            # plt.show()\n","\n","        optimizer.alpha *= lr_decay\n","        cur_at = now\n","\n","    # model と optimizer を保存する\n","    if args.gpu >= 0: model.to_cpu()\n","    chainer.serializers.save_npz(os.path.join(args.out, 'final.model'), model)\n","    chainer.serializers.save_npz(os.path.join(args.out, 'final.state'), optimizer)\n","    if args.gpu >= 0: model.to_gpu()\n","\n","    # test\n","    print('loading early stopped-model at epoch {}'.format(min_epoch))\n","    chainer.serializers.load_npz(os.path.join(args.out, 'early_stopped.model'), model)\n","    sys.stdout.flush()\n","\n","    vocab = pickle.load(open(os.path.join(args.out, 'vocab.bin'), 'rb'))\n","    token2id = {}\n","    for i, token in enumerate(vocab):\n","        token2id[token] = i\n","\n","    with chainer.no_backprop_mode(), chainer.using_config('train', False):\n","        show_sample(model, vocab, token2id, length=500, eos=\"\\n\")\n","\n","    logger.info('time spent: {:.6f} sec\\n'.format(time.time() - start_time))\n","\n","    \n","if __name__ == '__main__':\n","    main()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DXtHrWcac6dx","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls -al ~/.local/share/jupyter/runtime"],"execution_count":0,"outputs":[]}]}